{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgfUjnlULis6ycEQuOxVQ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2003Yash/multi-head-attention/blob/main/Multi_head_attention_for_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi Head attention meachinism: very similar to self attention but in self attention we mix q,k,v values and use softmax to normalize to find the perfect context vectors. here we are using 8 attentin heads for each q,k,v vector and batch 30 simultaneously and use them to make word embedding of same dimension as input but more context aware\n"
      ],
      "metadata": {
        "id": "MSKw_jxy2DCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class MultiheadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_dim, d_model, num_heads):\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads # creating head size => 512 / 8 => each embedding is divided for 8 attention head each head dim = 64 # // means same as / but result will be an int for / result will be an float\n",
        "        self.qkv_layer = tf.keras.layers.Dense(3 * d_model) # we are multiplying data into 3 so that q,k,v vectors are genreated => 512x3 = 1536\n",
        "        self.linear_layer = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        batch_size, sequence_length, input_dim = tf.shape(x)\n",
        "        print(f\"x.shape: {x.shape}\")\n",
        "\n",
        "        qkv = self.qkv_layer(x)\n",
        "        print(f\"qkv.shape: {qkv.shape}\")\n",
        "\n",
        "        qkv = tf.reshape(qkv, (batch_size, sequence_length, self.num_heads, 3 * self.head_dim)) # reshaping so we could process each head sperately i.e head = 64\n",
        "        print(f\"qkv.shape: {qkv.shape}\")\n",
        "\n",
        "        qkv = tf.transpose(qkv, perm=[0, 2, 1, 3]) # since it's a tensor of 3 dimensions we can't use transpose function , rather we should specify how we transpose  ## syntax [0,1,2,3] = no transpose\n",
        "        print(f\"qkv.shape: {qkv.shape}\")\n",
        "\n",
        "        q, k, v = tf.split(qkv, 3, axis=-1) # spliting vector into q, k, v vectors\n",
        "        print(f\"q.shape: {q.shape}, k.shape: {k.shape}, v.shape: {v.shape}\")\n",
        "\n",
        "        values, attention = self.scaled_dot_product(q, k, v, mask)\n",
        "        print(f\"values.shape: {values.shape}, attention.shape: {attention.shape}\")\n",
        "\n",
        "        values = tf.reshape(values, (batch_size, sequence_length, self.num_heads * self.head_dim)) # reshape them back by mixing outputs of each head and no.of heads\n",
        "        print(f\"values.shape: {values.shape}\")\n",
        "\n",
        "        out = self.linear_layer(values)\n",
        "        print(f\"out.shape: {out.shape}\")\n",
        "        return out\n",
        "\n",
        "# to use mask -- we just have to create an numpy lowet triangular matrix and assign it toa  var mask1 and use it function by mask = mask1\n",
        "  # process is very similar to self attention\n",
        "    def scaled_dot_product(self, q, k, v, mask):\n",
        "        d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scores = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            scores += (mask * -1e9)\n",
        "        attention = tf.nn.softmax(scores, axis=-1)\n",
        "        values = tf.matmul(attention, v)\n",
        "        return values, attention"
      ],
      "metadata": {
        "id": "Ry9D7LA9iytN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 512 # this represents input for attention unit size which is the embedding matrix size of each word\n",
        "d_model = 512 # this represents the output from attention unit which is the embedding matrix size of each word\n",
        "num_heads = 8 # specified as per Attention is all you need Research paper\n",
        "\n",
        "batch_size = 30 # we will simulatneous do mini batch learning of 30 values each time\n",
        "sequence_length = 4 # no.of words in a sentence\n",
        "x = tf.random.normal((batch_size, sequence_length, input_dim)) #random sample of word embedding data data over the given dimensions\n",
        "# x is value we made after pos encoding and and x is feeded to multi head attention in encoder\n",
        "\n",
        "model = MultiheadAttention(input_dim, d_model, num_heads) # creating an object of class\n",
        "out = model(x) # using object to perfrom multi head attention\n",
        "\n",
        "#even if iput size is 1024 we get output as 512 dimensions since d_model value is 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPbA1FVyi5er",
        "outputId": "736c95c8-a5d1-4d22-eaef-f9dba6b5c1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape: (30, 4, 512)\n",
            "qkv.shape: (30, 4, 1536)\n",
            "qkv.shape: (30, 4, 8, 192)\n",
            "qkv.shape: (30, 8, 4, 192)\n",
            "q.shape: (30, 8, 4, 64), k.shape: (30, 8, 4, 64), v.shape: (30, 8, 4, 64)\n",
            "values.shape: (30, 8, 4, 64), attention.shape: (30, 8, 4, 4)\n",
            "values.shape: (30, 4, 512)\n",
            "out.shape: (30, 4, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "take input (512) -> create q,k,v vectors (512 x 3 = 1536) -> put them qkv neural network layer (learnable) -> divide them over the 8 heads (1536/8 = 192) -> transpose it (head and no.of sequences are inverteed) so we can easily perform parallel processing on sentemces i.e last 2 dimensions-> split them to q, k, v seperately -> (if decoder then use mask here) -> get attention matix and use it to get values( new more context aware embedding values) ->  transpose values back to original shape by mixing outputs of each head and no.of heads -> put them through linear layer nueral network of d_model nodes which is learnable -> and we get output which is basically the same shape as linear layer and it is output of attention block\n"
      ],
      "metadata": {
        "id": "OYjn6S_5425k"
      }
    }
  ]
}